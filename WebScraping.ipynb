{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 'NoneType' object has no attribute 'find_all'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "from lxml import etree\n",
    "class ECNS(object):\n",
    "    def __init__(self):\n",
    "        self.url = ['http://www.ecns.cn/scroll/scroll.d.html?nid=89&p={page}',#wire 15'http://www.ecns.cn/travel/more.d.html?nid=93&p={page}',#travel 2\n",
    "                    'http://www.ecns.cn/news/more.d.html?nid=133&p={page}',# culture 8\n",
    "                    'http://www.ecns.cn/news/more.d.html?nid=98&p={page}',#business\n",
    "                    'http://www.ecns.cn/scroll/scroll.d.html?nid=89&p={page}',#latest news 10\n",
    "                    'http://www.ecns.cn/news/more.d.html?nid=125&p={page}',#science and tech\n",
    "                    ]\n",
    "        self.header = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36'\n",
    "            }\n",
    "\n",
    "    #获取每一页的连接\n",
    "    def get_url(self,url):\n",
    "        ul = []\n",
    "        # to get url, mimic to request server.\n",
    "        res = requests.get(url)\n",
    "        # point out the interpreter of BS is 'html.parser'.\n",
    "        soup = BeautifulSoup(res.text,'html.parser')\n",
    "        # find the pos of href.\n",
    "        listing = soup.find('ul', id = 'lastline')\n",
    "        for i in listing.find_all('ul', id = 'lastline'):\n",
    "            # get all href.\n",
    "            news_url = i.a['href']\n",
    "            ul.append(news_url)\n",
    "        return ul\n",
    "        \n",
    "    #获取内容\n",
    "    def get_content(self, url):\n",
    "            r = requests.get(url)\n",
    "            image = []\n",
    "            pic_txt = []\n",
    "            content = ''\n",
    "            date = []\n",
    "            title = []\n",
    "            source = []\n",
    "            # gener = []\n",
    "            # author = []\n",
    "            if r.status_code == 200:\n",
    "                res = requests.get(url)\n",
    "                soup = BeautifulSoup(res.text,'html.parser')\n",
    "                # which means we are access to the website.\n",
    "                first_tag = soup.find('ul', id = 'lastline')\n",
    "                # rt = r.text\n",
    "                # xp = etree.HTML(rt)\n",
    "                # href: pic_txt.append(row.find('a'))\n",
    "                for row in first_tag.find_all('ul', class_ = ['bizlst dashedlne mart5 overhid']):\n",
    "                    # ??? image\n",
    "                    # add all correspond image and images' text.\n",
    "                    image.append(row.find('img')['src'])\n",
    "                    newPicText = row.find('span', class_ = 'floatrgt').a['href']\n",
    "                    pic_txt.append(newPicText)\n",
    "                    # gain content of news.\n",
    "                    c = soup.find('li')\n",
    "                    for i in c.find_all('p')[1:-1]:\n",
    "                        content = content+' '+i.text.strip()+'\\n'\n",
    "                    # gain date of news.\n",
    "                    date.append(row.find('b').text.strip())\n",
    "                    # gain title of news.\n",
    "                    title.append(row.find('h3').a.text.strip())\n",
    "                    # gain source of news.\n",
    "                    source.append(row.find('h3').a['href'])\n",
    "                news_t = [image, pic_txt, content, date, title, source]\n",
    "                print(news_t)\n",
    "                self.save_img(news_t)\n",
    "                self.save_pic_txt(news_t)\n",
    "                self.save_content(news_t)\n",
    "                self.save_date(news_t)\n",
    "                self.save_title(news_t)\n",
    "                    \n",
    "                \n",
    "\n",
    "    # save image.\n",
    "    def save_img(self,new_t):\n",
    "        with open('image.csv', 'a', newline='', encoding='utf-8') as f:\n",
    "            for i in range(len(new_t)):\n",
    "                writer = csv.writer(f)\n",
    "                image = new_t[0]\n",
    "                source = new_t[5]\n",
    "                L = [image, source]\n",
    "                writer.writerow(L)\n",
    "                print('Saved: {}'.format(source))\n",
    "\n",
    "    # save pic_txt.\n",
    "    def save_pic_txt(self,new_t):\n",
    "        with open('pic_txt.csv', 'a', newline='', encoding='utf-8') as f:\n",
    "            for i in range(len(new_t)):\n",
    "                writer = csv.writer(f)\n",
    "                pic_txt = new_t[1]\n",
    "                source = new_t[5]\n",
    "                L = [pic_txt, source]\n",
    "                writer.writerow(L)\n",
    "                print('Saved: {}'.format(source))\n",
    "    \n",
    "    # save content.\n",
    "    def save_content(self,new_t):\n",
    "        with open('content.csv', 'a', newline='', encoding='utf-8') as f:\n",
    "            for i in range(len(new_t)):\n",
    "                writer = csv.writer(f)\n",
    "                content = new_t[2]\n",
    "                source = new_t[5]\n",
    "                L = [content, source]\n",
    "                writer.writerow(L)\n",
    "                print('Saved: {}'.format(source))\n",
    "\n",
    "    # save date.\n",
    "    def save_date(self,new_t):\n",
    "        with open('date.csv', 'a', newline='', encoding='utf-8') as f:\n",
    "            for i in range(len(new_t)):\n",
    "                writer = csv.writer(f)\n",
    "                date = new_t[3]\n",
    "                source = new_t[5]\n",
    "                L = [date, source]\n",
    "                writer.writerow(L)\n",
    "                print('Saved: {}'.format(source))\n",
    "\n",
    "    # save title.\n",
    "    def save_title(self,new_t):\n",
    "        with open('title.csv', 'a', newline='', encoding='utf-8') as f:\n",
    "            for i in range(len(new_t)):\n",
    "                writer = csv.writer(f)\n",
    "                title = new_t[4]\n",
    "                source = new_t[5]\n",
    "                L = [title, source]\n",
    "                writer.writerow(L)\n",
    "                print('Saved: {}'.format(source))\n",
    "\n",
    "    # run all code.\n",
    "    def run(self):\n",
    "        linklist = []\n",
    "        #按页数访问\n",
    "        for i in self.url:\n",
    "            for j in [str(x) for x in range(1,16)]:\n",
    "                u = i.format(page = j)\n",
    "                ul = self.get_url(url=u)\n",
    "                for k in ul:\n",
    "                    linklist.append(k)\n",
    "                    self.get_content(url=k)\n",
    "        print(len(linklist))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # catch the Exception\n",
    "    try:\n",
    "        spider = ECNS()\n",
    "        spider.run()\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.ecns.cn/news/2022-04-30/detail-ihaxwerf5653436.shtml\n",
      "http://www.ecns.cn/news/2022-04-30/detail-ihaxwerf5653428.shtml\n",
      "http://www.ecns.cn/news/2022-04-30/detail-ihaxwerf5653426.shtml\n",
      "http://www.ecns.cn/news/2022-04-30/detail-ihaxwerf5653134.shtml\n",
      "http://www.ecns.cn/news/2022-04-30/detail-ihaxwerf5653136.shtml\n",
      "http://www.ecns.cn/news/2022-04-30/detail-ihaxwerf5653132.shtml\n",
      "http://www.ecns.cn/news/2022-04-30/detail-ihaxwerf5653122.shtml\n",
      "http://www.ecns.cn/news/2022-04-30/detail-ihaxwerf5653124.shtml\n",
      "http://www.ecns.cn/news/2022-04-30/detail-ihaxwerf5653120.shtml\n",
      "http://www.ecns.cn/news/economy/2022-04-30/detail-ihaxwerf5653087.shtml\n",
      "http://www.ecns.cn/news/society/2022-04-29/detail-ihaxwerf5653082.shtml\n",
      "http://www.ecns.cn/news/2022-04-29/detail-ihaxwerf5652864.shtml\n",
      "http://www.ecns.cn/news/2022-04-29/detail-ihaxwerf5652852.shtml\n",
      "http://www.ecns.cn/news/society/2022-04-29/detail-ihaxwerf5652643.shtml\n",
      "http://www.ecns.cn/news/2022-04-29/detail-ihaxwerf5652510.shtml\n",
      "http://www.ecns.cn/news/2022-04-29/detail-ihaxwerf5652416.shtml\n",
      "http://www.ecns.cn/news/cns-wire/2022-04-29/detail-ihaxwerf5652352.shtml\n",
      "http://www.ecns.cn/news/2022-04-29/detail-ihaxwerf5652199.shtml\n",
      "http://www.ecns.cn/photo/2022-04-29/detail-ihaxwerf5652110.shtml\n",
      "http://www.ecns.cn/news/cns-wire/2022-04-29/detail-ihaxwerf5651830.shtml\n",
      "http://www.ecns.cn/news/cns-wire/2022-04-29/detail-ihaxwerf5651707.shtml\n",
      "http://www.ecns.cn/news/cns-wire/2022-04-29/detail-ihaxwerf5651702.shtml\n",
      "http://www.ecns.cn/news/2022-04-29/detail-ihaxwerf5651692.shtml\n",
      "http://www.ecns.cn/news/2022-04-29/detail-ihaxwerf5651682.shtml\n",
      "http://www.ecns.cn/news/2022-04-29/detail-ihaxwerf5651680.shtml\n",
      "http://www.ecns.cn/video/2022-04-29/detail-ihaxwerf5651618.shtml\n",
      "http://www.ecns.cn/video/2022-04-29/detail-ihaxwerf5651616.shtml\n",
      "http://www.ecns.cn/video/2022-04-29/detail-ihaxwerf5651614.shtml\n",
      "http://www.ecns.cn/video/2022-04-29/detail-ihaxwerf5651612.shtml\n",
      "http://www.ecns.cn/news/2022-04-29/detail-ihaxwerf5651584.shtml\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import pymysql\n",
    "from lxml import etree\n",
    "\n",
    "url_list = []\n",
    "url = 'http://www.ecns.cn/scroll/scroll.d.html?nid=89&p=12'\n",
    "r = requests.get(url)\n",
    "rt = r.text\n",
    "soup = BeautifulSoup(rt,'html.parser')\n",
    "t = soup.find_all('ul',id = 'lastline')\n",
    "for i in t[1:]:\n",
    "    li = i.a['href']\n",
    "    url_list.append(li)\n",
    "    print(li)\n",
    "#/html/body/div[3]/div[1]/ul/ul[3]/li/h3/a\n",
    "#/html/body/div[3]/div[1]/ul/ul[1]/li/h3/a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Economy\n",
      "Analysts divided on U.S. economic prospect\n",
      "  by Matthew Rusling\n",
      "  Friday marked the end of a super volatile U.S. trading week. Some experts and analysts maintain that topsy-turvy markets are a sign of bad economic times ahead, while others believe the future is bright.\n",
      "  MARKET FLUCTUATION\n",
      "  The Dow Jones Industrial Average tanked by more than 800 points by the end of trading Tuesday, then surged more than 600 points Thursday before taking another nosedive more than 900 points by end of trading Friday.\n",
      "  Thursday's rally came unexpectedly - ironically on news that U.S. gross domestic product declined at an annualized 1.4 percent in the first quarter.\n",
      "  Some analysts chalked up the market's reaction to the possibility that the news could cause the U.S. Federal Reserve to hike interest rates less aggressively, in its efforts to control the highest inflation in 40 years.\n",
      "  Paul Reilly, CEO of Raymond James Financial, chalked up the negative first quarter report to supply chain bottlenecks impacting U.S. sales, imports being up and exports being down.\n",
      "  \"If those were more normalized, we would have had positive GDP,\" he said, as quoted in Yahoo Finance.\n",
      "  Noting that interest rates are important to go up to fight inflation, Reilly said, \"The problem is we should have started sooner and we'll have to do it quicker, which is going to create some disruption.\"\n",
      "  MOOD OF PESSIMISM\n",
      "  Desmond Lachman, a resident fellow at the American Enterprise Institute, told Xinhua the stock market's 12 percent decline since the start of the year has been a reflection of the Fed's shift to a more hawkish monetary policy stance.\n",
      "  Markets are beginning to anticipate that the stock market will decline as the Fed starts an interest rate hiking cycle, and also starting to fear that the Fed will not succeed in squeezing inflation out of the economy without triggering a recession, Lachman said.\n",
      "  Gary Hufbauer, nonresident senior fellow at the Peterson Institute for International Economics, told Xinhua that there's no historical experience that suggests with such high inflation, the Federal Reserve is able to bring inflation down to its 2-percent goal without a recession.\n",
      "  \"We're gonna have a recession,\" said Hufbauer, a former U.S. Treasury official. \"And the only question is when the recession really starts.\"\n",
      "  \"Now we had this result for this quarter, most people are saying that next quarter will be positive, so you won't have two (negative) quarters in a row,\" Hufbauer said. \"But I think by the end of this year, say in the fourth quarter of 2022 in the first quarter of 2023, a recession is very likely.\"\n",
      "  OPTIMISTIC NARRATIVES\n",
      "  However, other economists expect different trends.\n",
      "  Bernard Baumohl, chief global economist at The Economic Outlook Group, told Xinhua, \"I'm not at all concerned about the U.S. having a recession this point.\"\n",
      "  The negative GDP quarter was due to factors including net imports being more than exports, which caused a rise in the U.S. trade deficit and subtracted from U.S. growth, said Baumohl.\n",
      "  In his view, Americans are spending the collective trillions of U.S. dollars they saved during the pandemic, as travel is surging back and people have returned to shopping malls, bars and restaurants. That spending has contributed to a U.S. trade deficit.\n",
      "  \"The underlying fact is this - Americans are spending, businesses are investing,\" although companies are not able to invest in inventories as much due in part to lingering supply chain issues, Baumohl said.\n",
      "  \"Don't freak out about the GDP report, the underlying inertial components were strong,\" Jason Furman, former chairman of the White House Council of Economic Advisers, tweeted on Thursday.\n",
      "  \"The headline was - 1.4 pct growth at an annual rate. BUT, inventories subtracted 0.8pp (percentage point) and net exports subtracted 3.2pp. Consumption, fixed investment, and key domestic demand components strong,\" he said.\n",
      "  As for the Fed, the institution must navigate a complex road ahead.\n",
      "  \"It's a hellishly complicated matter for the Fed. They want to bring inflation down, but they also realize that we had negative GDP growth. Therefore, they're likely going to be a little bit more cautious in their decision as to when and how much to raise short-term rates,\" Baumohl said.\n",
      "  \"You don't want to hit the brakes too hard when the car's already slowing down,\" he said.\n",
      "  \n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pic_txt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9772cc0f7c97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'  '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtitile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpic_txt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pic_txt' is not defined"
     ]
    }
   ],
   "source": [
    "for url in url_list:\n",
    "    r = requests.get(url)\n",
    "    print(r.status_code)\n",
    "    rt =r.text\n",
    "    soup = BeautifulSoup(rt,'html.parser')\n",
    "    xp = etree.HTML(rt)\n",
    "    image = ''\n",
    "    typel = xp.xpath('/html/body/div[3]/div[1]/h3/span/text()')\n",
    "    for i in typel:\n",
    "        type = i.strip()\n",
    "    print(type)\n",
    "    titilel = xp.xpath('//*[@id=\"contitle\"]/text()')\n",
    "    for i in titilel:\n",
    "        titile = i.strip()\n",
    "        print(titile)\n",
    "    pic = xp.xpath('//*[@id=\"yanse\"]/div/img')\n",
    "    for i in pic:\n",
    "        pn = i.attrib\n",
    "        image = pn['src']\n",
    "        print(image)\n",
    "    pic_t = xp.xpath('//*[@id=\"yanse\"]/div/div/text()')\n",
    "    for i in pic_t:\n",
    "        pic_txt = i\n",
    "    s = xp.xpath('/html/body/div[3]/div[1]/div[1]/span[2]/text()')\n",
    "    for i in s:\n",
    "        source = i\n",
    "    c = soup.find('div',class_='content',id = 'yanse')\n",
    "    content = ''\n",
    "    for i in c.find_all('p'):\n",
    "        content = content+'  '+i.text.strip()+'\\n'\n",
    "    print(content)\n",
    "    l = [type,titile,url,image,pic_txt,source,content]\n",
    "    with open('text.csv','a',newline='',encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-dc912fb1918e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'yanse'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'p'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcon\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "url = 'http://www.ecns.cn/hd/2021-12-10/detail-ihatptun5312699.shtml'\n",
    "r = requests.get(url)\n",
    "print(r.status_code)\n",
    "rt = r.text\n",
    "soup = BeautifulSoup(rt,'html.parser')\n",
    "c = soup.find('div',class_='content',id = 'yanse')\n",
    "con = c.find_all('p')\n",
    "for i in con[1:-1]:\n",
    "    print(i.text)\n",
    "d = xp.xpath('/html/body/div[3]/div[1]/div[1]/span[1]/text()')\n",
    "for i in d:\n",
    "    date = i[0:10]\n",
    "    print(date)\n",
    "#/html/body/div[3]/h3/span/text()\n",
    "\n",
    "    #http://www.ecns.cn/photo/2021-12-14/detail-ihattyiu7540278.shtml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# else version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "from lxml import etree\n",
    "class ECNS(object):\n",
    "    def __init__(self):\n",
    "        self.url = ['http://www.ecns.cn/scroll/scroll.d.html?nid=89&p={page}',# wire 15'http://www.ecns.cn/travel/more.d.html?nid=93&p={page}',#travel 2\n",
    "                    'http://www.ecns.cn/news/more.d.html?nid=133&p={page}',# culture 8\n",
    "                    'http://www.ecns.cn/news/more.d.html?nid=98&p={page}',# business\n",
    "                    'http://www.ecns.cn/scroll/scroll.d.html?nid=89&p={page}',# latest news 10\n",
    "                    'http://www.ecns.cn/news/more.d.html?nid=125&p={page}',# science and tech\n",
    "                    ]\n",
    "        self.header = {\n",
    "            'User-Agent': 'user-agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, '\n",
    "                          'like Gecko) Chrome/94.0.4606.81 Safari/537.36'}\n",
    "\n",
    "    #获取每一页的连接\n",
    "    def get_url(self,url):\n",
    "        ul = []\n",
    "        # to get url, mimic to request server.\n",
    "        res = requests.get(url)\n",
    "        # get text content in the url.\n",
    "        rt = res.text\n",
    "        # point out the interpreter of BS is 'html.parser'.\n",
    "        soup = BeautifulSoup(rt,'html.parser')\n",
    "        # find the pos of href.\n",
    "        t = soup.find_all('ul',id = 'lastline')\n",
    "        for i in t[1: ]:\n",
    "            # get all href.\n",
    "            news_url = i.a['href']\n",
    "            ul.append(news_url)\n",
    "        return ul\n",
    "\n",
    "    #获取内容\n",
    "    def get_content(self,url):\n",
    "            r = requests.get(url)\n",
    "            image = ''\n",
    "            pic_txt = ''\n",
    "            gener = ''\n",
    "            content = ''\n",
    "            author = ''\n",
    "            date = ''\n",
    "            title = ''\n",
    "            source = ''\n",
    "            if r.status_code == 200:\n",
    "                # which means we are access to the website.\n",
    "                rt = r.text\n",
    "                xp = etree.HTML(rt)\n",
    "                er = xp.xpath('/html/body/div[3]/h3/span/text()')\n",
    "                #???if\n",
    "                if 'Photo' not in er:\n",
    "                    rt = r.text\n",
    "                    soup = BeautifulSoup(rt,'html.parser')\n",
    "                    #获取种类\n",
    "                    generl = xp.xpath('/html/body/div[3]/div[1]/h3/span/text()')\n",
    "                    for i in generl:\n",
    "                        gener = i.strip()\n",
    "                    #获取标题\n",
    "                    tilel = xp.xpath('//*[@id=\"contitle\"]/text()')\n",
    "                    for i in tilel:\n",
    "                        title = i.strip()\n",
    "                    #获取图片\n",
    "                    pic = xp.xpath('//*[@id=\"yanse\"]/div/img')\n",
    "                    for i in pic:\n",
    "                        pn = i.attrib\n",
    "                        image = pn['src']\n",
    "                    pic_t = xp.xpath('//*[@id=\"yanse\"]/div/div/text()')\n",
    "                    #获取图片文本\n",
    "                    for i in pic_t:\n",
    "                        pic_txt = i\n",
    "                    #获取来源\n",
    "                    s = xp.xpath('/html/body/div[3]/div[1]/div[1]/span[2]/text()')\n",
    "                    for i in s:\n",
    "                        source = i\n",
    "                    #获取新闻内容\n",
    "                    c = soup.find('div',class_='content',id = 'yanse')\n",
    "                    for i in c.find_all('p')[1:-1]:\n",
    "                        content = content+'  '+i.text.strip()+'\\n'\n",
    "                    #获取作者\n",
    "                    a = xp.xpath('/html/body/div[3]/div[1]/div[1]/span[3]/text()')\n",
    "                    for i in a:\n",
    "                        author = i\n",
    "                    #获得日期\n",
    "                    d = xp.xpath('/html/body/div[3]/div[1]/div[1]/span[1]/text()')\n",
    "                    for i in d:\n",
    "                        date = i[0:10]\n",
    "                    news_t = [gener,title,url,date,author,content,source,image,pic_txt]\n",
    "                    print(news_t)\n",
    "                    self.save(news_t)\n",
    "\n",
    "    #保存\n",
    "    def save(self,new_t):\n",
    "        with open('ECNS.csv', 'a', newline='', encoding='utf-8') as f:\n",
    "            for i in range(len(new_t)):\n",
    "                writer = csv.writer(f)\n",
    "                genre = new_t[0]\n",
    "                title = new_t[1]\n",
    "                href = new_t[2]\n",
    "                time = new_t[3]\n",
    "                atr = new_t[4]\n",
    "                news = new_t[5]\n",
    "                source = new_t[6]\n",
    "                picture = new_t[7]\n",
    "                pic_text = new_t[8]\n",
    "                L = [genre, title, href, time, atr, news, source, picture, pic_text]\n",
    "                writer.writerow(L)\n",
    "                print('已收录：{}'.format(href))\n",
    "    #运行\n",
    "    def run(self):\n",
    "        linklist = []\n",
    "        #按页数访问\n",
    "        for i in self.url:\n",
    "            for j in [str(x) for x in  range(1,16)]:\n",
    "                u = i.format(page = j)\n",
    "                ul = self.get_url(url=u)\n",
    "                for k in ul:\n",
    "                    linklist.append(k)\n",
    "                    self.get_content(url=k)\n",
    "        print(len(linklist))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # catch the Exception\n",
    "    try:\n",
    "        spider = ECNS()\n",
    "        spider.run()\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import pymysql\n",
    "from lxml import etree\n",
    "\n",
    "url_list = []\n",
    "url = 'http://www.ecns.cn/scroll/scroll.d.html?nid=89&p=12'\n",
    "r = requests.get(url)\n",
    "rt = r.text\n",
    "soup = BeautifulSoup(rt,'html.parser')\n",
    "t = soup.find_all('ul',id = 'lastline')\n",
    "for i in t[1:]:\n",
    "    li = i.a['href']\n",
    "    url_list.append(li)\n",
    "    print(li)\n",
    "#/html/body/div[3]/div[1]/ul/ul[3]/li/h3/a\n",
    "#/html/body/div[3]/div[1]/ul/ul[1]/li/h3/a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in url_list:\n",
    "    r = requests.get(url)\n",
    "    print(r.status_code)\n",
    "    rt =r.text\n",
    "    soup = BeautifulSoup(rt,'html.parser')\n",
    "    xp = etree.HTML(rt)\n",
    "    image = ''\n",
    "    typel = xp.xpath('/html/body/div[3]/div[1]/h3/span/text()')\n",
    "    for i in typel:\n",
    "        type = i.strip()\n",
    "    print(type)\n",
    "    titilel = xp.xpath('//*[@id=\"contitle\"]/text()')\n",
    "    for i in titilel:\n",
    "        titile = i.strip()\n",
    "        print(titile)\n",
    "    pic = xp.xpath('//*[@id=\"yanse\"]/div/img')\n",
    "    for i in pic:\n",
    "        pn = i.attrib\n",
    "        image = pn['src']\n",
    "        print(image)\n",
    "    pic_t = xp.xpath('//*[@id=\"yanse\"]/div/div/text()')\n",
    "    for i in pic_t:\n",
    "        pic_txt = i\n",
    "    s = xp.xpath('/html/body/div[3]/div[1]/div[1]/span[2]/text()')\n",
    "    for i in s:\n",
    "        source = i\n",
    "    c = soup.find('div',class_='content',id = 'yanse')\n",
    "    content = ''\n",
    "    for i in c.find_all('p'):\n",
    "        content = content+'  '+i.text.strip()+'\\n'\n",
    "    print(content)\n",
    "    l = [type,titile,url,image,pic_txt,source,content]\n",
    "    with open('text.csv','a',newline='',encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.ecns.cn/hd/2021-12-10/detail-ihatptun5312699.shtml'\n",
    "r = requests.get(url)\n",
    "print(r.status_code)\n",
    "rt = r.text\n",
    "soup = BeautifulSoup(rt,'html.parser')\n",
    "c = soup.find('div',class_='content',id = 'yanse')\n",
    "con = c.find_all('p')\n",
    "for i in con[1:-1]:\n",
    "    print(i.text)\n",
    "d = xp.xpath('/html/body/div[3]/div[1]/div[1]/span[1]/text()')\n",
    "for i in d:\n",
    "    date = i[0:10]\n",
    "    print(date)\n",
    "#/html/body/div[3]/h3/span/text()\n",
    "\n",
    "    #http://www.ecns.cn/photo/2021-12-14/detail-ihattyiu7540278.shtml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "新上房源建业春天里电梯洋房，本小区 豪华的一套装修。  广告 4 室 2 厅 2 卫 160㎡ 南 高层(共9层) 2020年建造 建业春天里(川汇) 城区 市中心 开元大道 满二年 李凯云 3.7分 河南大都置业有限公司 125 万 7813元/㎡ .\n",
      "\n",
      "急卖 便宜出手 金色家园  毛坯大三房 有证  3 室 2 厅 2 卫 138㎡ 南北 中层(共26层) 2016年建造 金色家园(商水) 城区 市中心 阳城大道 满二唯一 新上 姜魁 4.4分 凯达房产 59 万 4276元/㎡ .\n",
      "\n",
      "业主急售.户型明亮通透.采光充足.观景房  4 室 2 厅 2 卫 141㎡ 东南 高层(共27层) 2018年建造 金色家园(商水) 城区 市中心 阳城大道 唯一住房 新上 朱爱花 4.4分 百辉地产 67 万 4752元/㎡ .\n",
      "\n",
      "急售融辉城二期 51万买通透三室 精装修家电家具全送 有钥匙  3 室 2 厅 2 卫 114.5㎡ 南北 中层(共6层) 2012年建造 蓝湾半岛 城区 市中心 中州大道 满二唯一 新上 祁文凤 4.1分 百辉地产 51 万 4455元/㎡ .\n",
      "\n",
      "降价急售 美景康城毛坯大三房44W 有证可按揭 随时看房！  3 室 2 厅 1 卫 121.2㎡ 南北 中层(共17层) 2010年建造 美景康城小区 城区 市中心 迎宾大道 满二唯一 雷向朋 3.8分 凯达房产 46 万 3797元/㎡ .\n",
      "\n",
      "南洋国际公馆精装大三房 有证可按揭 随时看房  3 室 2 厅 2 卫 138㎡ 南北 高层(共25层) 2010年建造 南洋国际公馆 城区 市中心 阳城大道 满二唯一 雷向朋 3.8分 凯达房产 54 万 3914元/㎡ .\n",
      "\n",
      "降价急售 美景康城毛坯大三房44W 有证可按揭 随时看房！  3 室 2 厅 1 卫 121.2㎡ 南北 中层(共17层) 2010年建造 美景康城小区 城区 市中心 迎宾大道 满二唯一 雷向朋 3.8分 凯达房产 46 万 3797元/㎡ .\n",
      "\n",
      "南洋国际公馆精装大三房 有证可按揭 随时看房  3 室 2 厅 2 卫 138㎡ 南北 高层(共25层) 2010年建造 南洋国际公馆 城区 市中心 阳城大道 满二唯一 雷向朋 3.8分 凯达房产 54 万 3914元/㎡ .\n",
      "\n",
      "融辉城二期标准三房带平台，低于市场价，可正常按揭  3 室 2 厅 2 卫 124㎡ 南北 低层(共17层) 2014年建造 蓝湾半岛 城区 市中心 中州大道 满二年 新上 凌霄 4.4分 恒嘉房产 53 万 4275元/㎡ .\n",
      "\n",
      "行政路和谐家园有证大三房全款36万，只卖几天  3 室 2 厅 1 卫 134㎡ 东西 高层(共12层) 2015年建造 和谐家园(商水) 城区 市中心 行政路 满二年 新上 凌霄 4.4分 恒嘉房产 36 万 2687元/㎡ .\n",
      "\n",
      "法姬娜一期电梯大三房，129平50万，有证可按揭，欢迎上车  3 室 2 厅 2 卫 129㎡ 南北 高层(共11层) 2014年建造 法姬娜欧洲城一期(商水) 城区 市中心 阳城大道 满二年 新上 凌霄 4.4分 恒嘉房产 50 万 3876元/㎡ .\n",
      "\n",
      "急售！！！有证满两年首辅不到8万买融辉城精装修大两房  2 室 2 厅 1 卫 85㎡ 南北 高层(共17层) 2017年建造 融辉城(一期) 城区 市中心 中州大道 满二唯一 李亚龙 4.5分 百辉地产 38.5 万 4530元/㎡ .\n",
      "\n",
      "急售！有证满两年融辉城一期一口价38.5万，有钥匙随时看房  2 室 2 厅 1 卫 85㎡ 南北 高层(共17层) 2017年建造 融辉城(一期) 城区 市中心 中州大道 满二唯一 李会婷 4.6分 百辉地产 38.5 万 4530元/㎡ .\n",
      "\n",
      "融辉城 洋房一楼带前后院 三室两厅两卫 有证可按揭 看房方便  3 室 2 厅 2 卫 130㎡ 南 共5层 2013年建造 融辉城(一期) 城区 市中心 中州大道 满五年 马彦红 4.4分 周口儒房地产 70 万 5385元/㎡ .\n",
      "\n",
      "吉祥湖旁 温泉小区荣盛社区 电梯大三房  看房方便  3 室 2 厅 2 卫 135㎡ 南北 高层(共7层) 2017年建造 荣盛社区 城区 市中心 106国道 满五唯一 王彩丽 3.9分 项城家惠房产 35 万 2593元/㎡ .\n",
      "\n",
      "金色家园电梯房，毛坯，57万.看房方便  3 室 2 厅 2 卫 141㎡ 南北 高层(共27层) 2016年建造 金色家园(商水) 城区 市中心 阳城大道 满二年 赵新荣 4.4分 百辉地产 59 万 4185元/㎡ .\n",
      "\n",
      "金色家园电梯房4楼.精装修拎包入住，随时看房  2 室 2 厅 1 卫 88㎡ 南北 低层(共26层) 2016年建造 金色家园(商水) 城区 市中心 阳城大道 满二年 赵新荣 4.4分 百辉地产 40 万 4546元/㎡ .\n",
      "\n",
      "融辉城二期电梯小高层东边户，送个衣帽间，有证可按揭  3 室 2 厅 2 卫 136㎡ 南 低层(共18层) 2017年建造 蓝湾半岛 城区 市中心 中州大道 满二唯一 张秋红 4.5分 百辉地产 63 万 4633元/㎡ .\n",
      "\n",
      "金色家园精装修大两室，家具家电全送，有证可按揭  2 室 2 厅 1 卫 88㎡ 南 低层(共27层) 2015年建造 金色家园(商水) 城区 市中心 阳城大道 满二唯一 张秋红 4.5分 百辉地产 40 万 4546元/㎡ .\n",
      "\n",
      "美景康城小区128.8平60万3室2厅2卫  3 室 2 厅 2 卫 128.8㎡ 南 低层(共17层) 2015年建造 美景康城小区 城区 市中心 迎宾大道 满二唯一 谭梦伟 4.4分 百辉地产 60 万 4659元/㎡ .\n",
      "\n",
      "此房是电梯房3楼毛坯，有证可按揭，随时看房  3 室 2 厅 2 卫 136㎡ 南北 低层(共18层) 2014年建造 蓝湾半岛 城区 市中心 中州大道 满二年 赵新荣 4.4分 百辉地产 62 万 4559元/㎡ .\n",
      "\n",
      "荣盛社区 一楼带院 带地下室 可领包入住 小区全年温泉水  3 室 2 厅 1 卫 110㎡ 南北 低层(共7层) 2018年建造 荣盛社区 城区 市中心 106国道 满二唯一 闻单单 4.2分 项城芒果地产 36 万 3273元/㎡ .\n",
      "\n",
      "电梯房 满二  住房 102平 印象康桥 采光好 周边商圈成  3 室 2 厅 1 卫 102㎡ 南北 高层(共11层) 2015年建造 印象康桥 城区 市中心 213省道 满二唯一 姜魁 4.4分 凯达房产 45 万 4412元/㎡ .\n",
      "\n",
      "融辉城一期 一楼带前后大院 户型方正 有证可按揭过户  3 室 2 厅 2 卫 130㎡ 南 共4层 2013年建造 融辉城(一期) 城区 市中心 中州大道 满二年 杨珍 4.7分 周口儒房地产 70 万 5385元/㎡ .\n",
      "\n",
      "碧水云天 多层2楼三房出售，南北通透 光线充足  3 室 2 厅 2 卫 133㎡ 南北 低层(共6层) 2013年建造 碧水云天 城区 市中心 纬一路 满二唯一 姜魁 4.4分 凯达房产 56 万 4211元/㎡ .\n",
      "\n",
      "融辉城(一期) 电梯房 87平  精装 南北通透 2室2厅  2 室 2 厅 1 卫 87.7㎡ 南北 中层(共11层) 2013年建造 融辉城(一期) 城区 市中心 中州大道 满二唯一 范芳 4.4分 周口二十一世纪房地产 39 万 4447元/㎡ .\n",
      "\n",
      "赔钱急用钱 商水金玉良苑3房两厅两卫大三房 有证可按揭有钥匙  3 室 2 厅 2 卫 145㎡ 南北 高层(共11层) 2019年建造 金玉良苑 城区 市中心 嵩阳大道 唯一住房 彭三红 4.5分 周口二十一世纪房地产 56 万 3863元/㎡ .\n",
      "\n",
      "法姬娜 电梯 装修两房 有证 可按揭  2 室 2 厅 1 卫 89㎡ 南北 高层(共18层) 2015年建造 法姬娜欧洲城一期(商水) 城区 市中心 阳城大道 满二唯一 姜魁 4.4分 凯达房产 42 万 4720元/㎡ .\n",
      "\n",
      "电梯房 锦园小区(商水)   住房 3室2厅 南北通透  3 室 2 厅 2 卫 124.4㎡ 南北 低层(共16层) 2014年建造 锦园小区(商水) 城区 市中心 惠商路 唯一住房 姜魁 4.4分 凯达房产 49.8 万 4004元/㎡ .\n",
      "\n",
      "万达广场商圈 新装未住三室两卫 装修用的都是号材料   即住  3 室 2 厅 2 卫 113.7㎡ 南北 低层(共17层) 2019年建造 建业春天里(川汇) 城区 市中心 开元大道 满二唯一 郭亚杰 4.5分 周口二十一世纪房地产 73 万 6424元/㎡ .\n",
      "\n",
      "法姬娜  电梯房 3室2厅 全天采光 一口价  3 室 2 厅 2 卫 120.5㎡ 南北 高层(共18层) 2016年建造 荣盛家园 城区 市中心 阳城大道 满二唯一 姜魁 4.4分 凯达房产 45 万 3735元/㎡ .\n",
      "\n",
      "好房出售 一楼80平南北双院 毛坯大三房 全天采光手续齐全  3 室 2 厅 2 卫 130㎡ 南北 共5层 2010年建造 融辉城(一期) 城区 市中心 中州大道 满二唯一 訾明杰 4.6分 周口二十一世纪房地产 70 万 5385元/㎡ .\n",
      "\n",
      "诚心出售 法姬娜二期电梯三房 有证可按揭  3 室 1 厅 2 卫 122㎡ 南北 高层(共18层) 2015年建造 荣盛家园 城区 市中心 阳城大道 满二唯一 姜魁 4.4分 凯达房产 48 万 3935元/㎡ .\n",
      "\n",
      "86平   住房 金色家园(商水) 2室2厅 朝南 房东急置  2 室 2 厅 1 卫 86.6㎡ 南 高层(共26层) 2015年建造 金色家园(商水) 城区 市中心 阳城大道 唯一住房 姜魁 4.4分 凯达房产 38 万 4390元/㎡ .\n",
      "\n",
      "融辉城 二期 大三房 南北通透户型，随时看房  3 室 2 厅 2 卫 132㎡ 南北 中层(共18层) 2016年建造 蓝湾半岛 城区 市中心 中州大道 满二唯一 姜魁 4.4分 凯达房产 63 万 4773元/㎡ .\n",
      "\n",
      "115平 满二  住房 印象康桥 南北通透 看房方便 有钥匙  3 室 2 厅 2 卫 115㎡ 南北 中层(共6层) 2015年建造 印象康桥 城区 市中心 213省道 满二唯一 姜魁 4.4分 凯达房产 51.5 万 4479元/㎡ .\n",
      "\n",
      "融辉城二期 电二楼带平台的  有证 可按揭。  3 室 2 厅 2 卫 124㎡ 南北 低层(共17层) 2016年建造 蓝湾半岛 城区 市中心 中州大道 满二唯一 姜魁 4.4分 凯达房产 55 万 4436元/㎡ .\n",
      "\n",
      "融辉城 南北通透 131平 3室2厅 精装修  诚心出售  3 室 2 厅 2 卫 131㎡ 南北 低层(共18层) 2016年建造 蓝湾半岛 城区 市中心 中州大道 满二唯一 郭亚杰 4.5分 周口二十一世纪房地产 69 万 5268元/㎡ .\n",
      "\n",
      "法姬娜欧洲城一期(商水) 2室2厅 86平 满二  2 室 2 厅 1 卫 86㎡ 南北 低层(共6层) 2006年建造 法姬娜欧洲城一期(商水) 城区 市中心 阳城大道 满二年 梁艳杰 3.8分 凯达房产 33.5 万 3896元/㎡ .\n",
      "\n",
      "此房是电梯7楼精装修，随时看房  3 室 2 厅 2 卫 114㎡ 南北 中层(共17层) 2016年建造 蓝湾半岛 城区 市中心 中州大道 满二年 赵新荣 4.4分 百辉地产 59 万 5176元/㎡ .\n",
      "\n",
      "精装修带地暖！融辉城二期南北通透大三房满两年可按揭  3 室 2 厅 2 卫 131㎡ 南北 低层(共26层) 2017年建造 蓝湾半岛 城区 市中心 中州大道 满二唯一 李亚龙 4.5分 百辉地产 72 万 5497元/㎡ .\n",
      "\n",
      "捡漏房源，一楼带院，采光超好，有证可按揭  3 室 2 厅 2 卫 122㎡ 南北 低层(共18层) 2020年建造 融辉理想城D1区 城区 市中心 宁洛高速 满二年 李亚龙 4.5分 百辉地产 58 万 4755元/㎡ .\n",
      "\n",
      "融辉城一期电梯新装，急售价格便宜随时看房  2 室 2 厅 1 卫 86㎡ 南北 高层(共16层) 2011年建造 融辉城(一期) 城区 市中心 中州大道 满二唯一 新上 凌霄 4.4分 恒嘉房产 38.5 万 4477元/㎡ .\n",
      "\n",
      "法姬娜欧洲城步梯二楼两室，南北通透，证满可按揭  2 室 2 厅 2 卫 86㎡ 东 低层(共6层) 2011年建造 法姬娜欧洲城一期(商水) 城区 市中心 阳城大道 满二年 凌霄 4.4分 恒嘉房产 33.5 万 3896元/㎡ .\n",
      "\n",
      "捡漏房源法姬娜大三房 120平45万一口价 单价3700一平  3 室 2 厅 2 卫 120㎡ 南北 高层(共18层) 2016年建造 法姬娜欧洲城一期(商水) 城区 市中心 阳城大道 满二唯一 李双双 3.8分 经纪人 48 万 4000元/㎡ .\n",
      "\n",
      "急售！有证！精装修！家具家电全送，边户，南北通透，户型方正  3 室 2 厅 2 卫 123㎡ 南北 低层(共18层) 2015年建造 蓝湾半岛 城区 市中心 中州大道 满二唯一 李双双 3.8分 经纪人 61 万 4960元/㎡ .\n",
      "\n",
      "简单装修 满二  住房 电梯房 3室2厅 朝南 利达佳园 采  3 室 2 厅 1 卫 109.1㎡ 南 低层(共11层) 2015年建造 利达佳园 城区 市中心 章华台路 满二唯一 赵凤勤 4.0分 经纪人 49 万 4491元/㎡ .\n",
      "\n",
      "碧桂园简单装修3室2厅2卫 总价低，朝向好！  3 室 2 厅 2 卫 128㎡ 南北 中层(共18层) 2019年建造 商水碧桂园 城区 市中心 汝阳路 凌霄 4.4分 恒嘉房产 63 万 4922元/㎡ .\n",
      "\n",
      "建业城一期简装大三房，采光  。家具家电齐全，拎包入住  3 室 2 厅 2 卫 139㎡ 南北 高层(共17层) 2019年建造 建业城(商水) 城区 市中心 商宁路 凌霄 4.4分 恒嘉房产 69 万 4965元/㎡ .\n",
      "\n",
      "毛坯两房 小高层 可根据自己喜欢风格随意装修  2 室 2 厅 1 卫 88.3㎡ 南北 高层(共11层) 2013年建造 法姬娜欧洲城一期(商水) 城区 市中心 阳城大道 满二唯一 李双双 3.8分 经纪人 41 万 4643元/㎡ .\n",
      "\n",
      "低调的奢华与张扬，深度的品质追求！法姬娜 三房大空间！  3 室 2 厅 2 卫 130㎡ 南 共5层 2015年建造 法姬娜欧洲城一期(商水) 城区 市中心 阳城大道 满二唯一 赵凤勤 4.0分 经纪人 55 万 4231元/㎡ .\n",
      "\n",
      "法姬娜欧洲城一期 一楼带院 满二    采光好 看房方便！！  2 室 2 厅 1 卫 87㎡ 南北 共5层 2015年建造 法姬娜欧洲城一期(商水) 城区 市中心 阳城大道 满二唯一 赵凤勤 4.0分 经纪人 40 万 4598元/㎡ .\n",
      "\n",
      "南北通透 3室2厅 碧水云天 143平 诚心出售 价格可议  3 室 2 厅 2 卫 143㎡ 南北 中层(共11层) 2003年建造 碧水云天 城区 市中心 纬一路 满二年 梁艳杰 3.8分 凯达房产 61.5 万 4301元/㎡ .\n",
      "\n",
      "南北通透 电梯房 锦园小区(商水) 3室2厅 房东急置换 诚  3 室 2 厅 2 卫 145㎡ 南北 低层(共16层) 2015年建造 锦园小区(商水) 城区 市中心 惠商路 满二年 姜魁 4.4分 凯达房产 59 万 4069元/㎡ .\n",
      "\n",
      "急售，建业春天里，三开间朝南，南北通透，户型方正，  3 室 2 厅 2 卫 135㎡ 西 低层(共17层) 2021年建造 建业春天里(川汇) 城区 市中心 开元大道 新上 胡凯兵 4.4分 百辉地产 76 万 5630元/㎡ .\n",
      "\n",
      "新上房源毛坯三房 一梯两户小高层 公摊小 价位低  3 室 2 厅 1 卫 113㎡ 南 中层(共11层) 2015年建造 瑞翔小区 城区 市中心 惠商路 满二唯一 李双双 3.8分 经纪人 50 万 4425元/㎡ .\n",
      "\n",
      "86平 满二  住房 法姬娜欧洲城 2室2厅 南北通透  2 室 2 厅 1 卫 86㎡ 南北 低层(共6层) 2013年建造 法姬娜欧洲城三期(商水) 城区 市中心 阳城大道 满二唯一 姜魁 4.4分 凯达房产 33.5 万 3896元/㎡ .\n",
      "\n",
      "捡漏房源，融辉城精装两室，有证可以按揭，看房有钥匙  2 室 2 厅 1 卫 86㎡ 南 中层(共17层) 2015年建造 融辉城(一期) 城区 市中心 中州大道 满二年 李会婷 4.6分 百辉地产 45 万 5233元/㎡ .\n",
      "\n",
      "润鸿悦榕府洋房社区，团购价欢迎咨询详情，价必须给力  3 室 2 厅 2 卫 129㎡ 南北 中层(共17层) 2021年建造 商水碧桂园 城区 市中心 汝阳路 凌霄 4.4分 恒嘉房产 52.5 万 4070元/㎡ .\n",
      "\n",
      "3室2厅 电梯房 金色家园(商水) 南北通透 看房方便 有钥  3 室 2 厅 2 卫 138㎡ 南北 中层(共26层) 2012年建造 金色家园(商水) 城区 市中心 阳城大道 满二唯一 新上 梁艳杰 3.8分 凯达房产 59 万 4276元/㎡ .\n",
      "\n",
      "融辉理想城D1区131.5平63万3室2厅2卫  3 室 2 厅 2 卫 131.5㎡ 南 中层(共18层) 2020年建造 融辉理想城D1区 城区 市中心 宁洛高速 满二唯一 新上 朱爱花 4.4分 百辉地产 63 万 4791元/㎡ .\n",
      "\n",
      "融辉理想城D1区南北双阳台有证可分期两卧朝南  3 室 2 厅 2 卫 131.5㎡ 南 中层(共18层) 2020年建造 融辉理想城D1区 城区 市中心 宁洛高速 满二唯一 张秋红 4.5分 百辉地产 63 万 4791元/㎡ .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from lxml import html\n",
    "etree = html.etree\n",
    "headers ={\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36 Edg/92.0.902.55'\n",
    "}\n",
    "url ='https://shangshui.58.com/ershoufang/?'\n",
    "page_text = requests.get(url=url,headers=headers).text\n",
    "#print(page_text)\n",
    "tree = etree.HTML(page_text)\n",
    "li_list = tree.xpath('//section[@class=\"list\"][1]/div')\n",
    "# print(li_list)\n",
    "fp = open('58.txt','w',encoding='utf-8')\n",
    "for li in li_list:\n",
    "    title = li.xpath('./a//div[@class=\"property-content-detail\"]//text()')\n",
    "    price= li.xpath('./a//div[@class=\"property-price\"]//text()')\n",
    "    # print(price)\n",
    "    titles=''\n",
    "    prices=''\n",
    "    for i in range(len(title)):\n",
    "\n",
    "        if title[i]!=', ' and title[i]!=' ':\n",
    "            titles += title[i].strip()\n",
    "            titles += \" \"\n",
    "        i+=1\n",
    "    for j in range(len(price)):\n",
    "        if price[j]!=', ' and price[j]!=' ':\n",
    "            prices +=price[j].strip()\n",
    "            prices +=\" \"\n",
    "        j+=1\n",
    "    titles_and_prices= titles+prices+'.\\n'\n",
    "    fp.write(titles_and_prices)\n",
    "    print(titles_and_prices)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "719d45e38b6b35dce474b00597395544d0fa9e2eb71949a83c153289e71e7b5f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
